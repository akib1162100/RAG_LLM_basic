{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afsarahannan/NLP_RAG_Project-/blob/main/Retrieval_Generation_project_with_ColBert_and_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# RAG Project: ColBert, T5-base and GPT3.5\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"T4 GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment, loading necessary libraries, load the datset"
      ],
      "metadata": {
        "id": "Mga1NykdObvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "2c4f2e83-c23b-4507-e195-67fc02ca684c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2662, done.\u001b[K\n",
            "remote: Counting objects: 100% (1165/1165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (364/364), done.\u001b[K\n",
            "remote: Total 2662 (delta 908), reused 849 (delta 801), pack-reused 1497\u001b[K\n",
            "Receiving objects: 100% (2662/2662), 2.03 MiB | 22.16 MiB/s, done.\n",
            "Resolving deltas: 100% (1667/1667), done.\n"
          ]
        }
      ],
      "source": [
        "#checking if the ColBert repository exists if not then clone the repo\n",
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "\n",
        "#import ColBert to the system path to import necessary modules\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmBi2UT5pxb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "095ca5ba-cbcb-49ad-ed7b-2b4e8ed44471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Obtaining file:///content/ColBERT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray (from colbert-ai==0.2.19)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting datasets (from colbert-ai==0.2.19)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (2.2.5)\n",
            "Collecting git-python (from colbert-ai==0.2.19)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.19)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.19)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (4.66.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (4.35.2)\n",
            "Collecting ujson (from colbert-ai==0.2.19)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting faiss-gpu>=1.7.0 (from colbert-ai==0.2.19)\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.13.1 (from colbert-ai==0.2.19)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->colbert-ai==0.2.19) (4.9.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.19) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.19) (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.4.1)\n",
            "Collecting multiprocess (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->colbert-ai==0.2.19) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (6.0.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (8.1.7)\n",
            "Collecting gitpython (from git-python->colbert-ai==0.2.19)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->colbert-ai==0.2.19) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (2024.2.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.19) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.19) (2023.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->colbert-ai==0.2.19) (1.16.0)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m601.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: ninja, faiss-gpu, bitarray, ujson, smmap, python-dotenv, pyarrow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, dill, nvidia-cudnn-cu11, multiprocess, gitdb, torch, gitpython, git-python, datasets, colbert-ai\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Running setup.py develop for colbert-ai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitarray-2.9.2 colbert-ai-0.2.19 datasets-2.17.1 dill-0.3.8 faiss-gpu-1.7.2 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.42 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyarrow-15.0.0 python-dotenv-1.0.1 smmap-5.0.1 torch-1.13.1 ujson-5.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#checks if the environment is Colab if so then install updated version of pip and Faiss GPU and pytorch\n",
        "try:\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "    #if not colab then add to the system path\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    #import Indexer and Searcher from the colbert package\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"You're running outside Colab, please make sure you install ColBERT in conda. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb6bd97-b84f-49a2-eb82-7e4721732940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "#import colbert and all the necessay packages from the library\n",
        "import colbert\n",
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the csv dataset here\n",
        "#note: remember to load the files in the temp content directory of colab\n",
        "import pandas as pd\n",
        "queries = pd.read_csv(\"/content/queries.csv\", header = None ,sep='\\t')\n",
        "answers = pd.read_csv(\"/content/answers.csv\", header = None ,sep='\\t')"
      ],
      "metadata": {
        "id": "grY5rJ27wVs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract all the questions and the answer passages from the documents\n",
        "\n",
        "questions =[x for x in queries[0]]\n",
        "answer_index = [x for x in queries[1]]\n",
        "answer_index_integers = [[int(num)-1 for num in sub_string.split(',')] for sub_string in answer_index]\n",
        "answer_text = [x for x in answers[0]]\n",
        "\n",
        "answer = [[answer_text[x] for x in sublist]for sublist in answer_index_integers]\n",
        "answer_compiled = [\"\".join(text) for text in answer]\n",
        "\n",
        "print(f\"There are {len(questions)} questions and {len(answer_text)} answers in this notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkG4eRoyYmm",
        "outputId": "b15cf709-7a4e-4da6-e3b1-01a8a6de72e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 190 questions and 203 answers in this notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The indexer module is responsible for building an index of the document embeddings which are representation of documents in a vector space.   \n",
        "The searcher module is responsible for performing search queries over the indexed document embeddings. The searcher returns the most relevant list of documents."
      ],
      "metadata": {
        "id": "llji7I_zU63C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "# encode each dimension with 2 bits because we are lowering the precision of the model to save the computational power which is limited in this case.\n",
        "#ColBert uses Byte Pair encoding (BPE)\n",
        "nbits = 2\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "# max_id = 10000\n",
        "\n",
        "index_name = f'ML_Edge.{nbits}bits'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "id": "OOvGGR4U6Mld",
        "outputId": "81ff4808-32d9-4f3a-b553-5da1a2677eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ML_Edge.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of this project we will not finetune the model because the dataset is small and the model might overfit the data. So the indexer will run on the dataset with the weights of the pretrained model.  "
      ],
      "metadata": {
        "id": "_BAHUU71arGG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRiOnzxtwI0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "97c8e1448c6541c69e391efdcfe29ad3",
            "05014f3cac2744dfac20f383a5e26054",
            "15b973809e424ae097506562ecc8295f",
            "7f014d52e21c44d282d5807e23cc4f85",
            "4ee5b8255da34544aba12175b33e85da",
            "5c08cb8731234e5c8d04913a0e763e11",
            "51adfa501baa4056948e3f539622dc64",
            "473fc769367b4093808c116125b0552b",
            "ece8c5dccbe94537a68a6d0f9fbeecb7",
            "6bbd2385204f4ca0b967b53b6acb2183",
            "567a627e6d3c485f924a114e4033c15d"
          ]
        },
        "outputId": "0db334a1-1d24-4639-861b-7d39ef503f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97c8e1448c6541c69e391efdcfe29ad3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[Feb 20, 09:26:17] #> Creating directory /content/experiments/notebook/indexes/ML_Edge.2bits \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "#> Joined...\n"
          ]
        }
      ],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "\n",
        "    # the indexer will load the checkpoint of the pre-trained ColBert model\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=answer_text, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The searcher module is responsible for embedding the query and then coducting similarity search after which it willl rank the indexed document and retrieve the document that has the highest similarity score with the query."
      ],
      "metadata": {
        "id": "4kAwplevcYTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3x_FnVnyB0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0739735-be97-4a15-a887-5ea45ff69b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 20, 09:35:41] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Feb 20, 09:35:41] #> Loading codec...\n",
            "[Feb 20, 09:35:41] #> Loading IVF...\n",
            "[Feb 20, 09:35:41] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 20, 09:36:13] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2918.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 20, 09:36:13] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 454.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 20, 09:36:13] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 20, 09:36:51] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        }
      ],
      "source": [
        "# Create the searcher\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "  searcher = Searcher(index=index_name, collection=answer_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I need the context list from the ColBert Model\n",
        "context = []\n",
        "\n",
        "for query in questions:\n",
        "  results = searcher.search(query, k=3)\n",
        "  intermediate_data = []\n",
        "  for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    data = searcher.collection[passage_id]\n",
        "    intermediate_data.append(data)\n",
        "  context.append(intermediate_data)\n",
        "  intermediate_data=[]\n",
        "\n",
        "context_compiled = [\"\".join(text) for text in context]"
      ],
      "metadata": {
        "id": "NRwrCNWriCl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21085ea-54ce-45d5-9a18-17b2abdca4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . what is machine learning ?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2054, 2003, 3698, 4083, 1029,  102,  103,  103,  103,  103,\n",
            "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
            "         103,  103,  103,  103,  103,  103,  103,  103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the context from ColBert model as an external file\n",
        "# File path to save the CSV file\n",
        "import csv\n",
        "file_path = '/content/context.txt'\n",
        "\n",
        "# Writing the list to a text file\n",
        "with open(file_path, 'w') as file:\n",
        "    for item in context:\n",
        "        file.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "ssMDihpLFOaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "mK6ouJpM7X2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation with pretrained t5-base model"
      ],
      "metadata": {
        "id": "puoVf0KJaeh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge\n"
      ],
      "metadata": {
        "id": "7saVRpLVaDO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow\n",
        "!pip uninstall -y evaluate\n",
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "4E7U4OU95MqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "# import evaluate\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SRWa7CqBVmGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the values for the model parameter\n",
        "MODEL_NAME =\"t5-base\"\n",
        "TOKENIZER = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "OPTIMIZER = Adam(MODEL.parameters(), lr=0.0001)\n",
        "Q_LEN = 150   # Question Length\n",
        "T_LEN = 200    # Target Length\n",
        "BATCH_SIZE = 3\n",
        "DEVICE = \"cpu\" #DEVICE = \"cuda:0\" #if you have cuda"
      ],
      "metadata": {
        "id": "pm6jjoMuVxld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class to extract the questions context and the answer from the dataframe\n",
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.q_len = q_len\n",
        "        self.t_len = t_len\n",
        "        self.data = dataframe\n",
        "        self.questions = self.data[\"questions\"]\n",
        "        self.context = self.data[\"context\"]\n",
        "        self.answer = self.data[\"answer\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        context = self.context[idx]\n",
        "        answer = self.answer[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
        "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n",
        "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "\n",
        "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": labels,\n",
        "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "A3eNZsBAX0an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_df = pd.DataFrame(questions, columns=['questions'])\n",
        "answer_df = pd.DataFrame(answer_compiled, columns=['answer'])\n",
        "context_df = pd.DataFrame(context_compiled, columns=['context'])\n",
        "\n",
        "data = pd.concat([questions_df, context_df,answer_df], axis=1)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QpLAmZ0MYJRB",
        "outputId": "f7f0af2d-3524-4186-fbb0-9b52d21fd19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             questions  \\\n",
              "0                           what is machine learning ?   \n",
              "1     Why do we need to know about machine learning ?    \n",
              "2                               What is deep learning?   \n",
              "3    How is deep learning different from machine le...   \n",
              "4    When do we use deep learning or machine learni...   \n",
              "..                                                 ...   \n",
              "185      What are the different types of quantization    \n",
              "186                   How does transfer learning work    \n",
              "187  What are the different types of transfer learning   \n",
              "188                    What is Knowledge Distillation    \n",
              "189              How does knowledge distillation work    \n",
              "\n",
              "                                               context  \\\n",
              "0    Machine learning is a branch of computer scien...   \n",
              "1    Machine Learning is an important field of data...   \n",
              "2    Deep learning is a subset of Machine learning ...   \n",
              "3    Deep learning is a subset of Machine learning ...   \n",
              "4    Deep learning is a subset of Machine learning ...   \n",
              "..                                                 ...   \n",
              "185  Quantization involves converting the continuou...   \n",
              "186  Transfer learning involves taking a model trai...   \n",
              "187  Types of Transfer Learning include Inductive T...   \n",
              "188  Knowledge distillation involves training a sma...   \n",
              "189  Knowledge distillation involves training a sma...   \n",
              "\n",
              "                                                answer  \n",
              "0    Machine learning is a branch of computer scien...  \n",
              "1    Machine Learning is an important field of data...  \n",
              "2    Deep learning is a subset of Machine learning ...  \n",
              "3    Unlike traditional ML  it does not require man...  \n",
              "4    Machine learning is used when the task is simp...  \n",
              "..                                                 ...  \n",
              "185  There is Post-Training Quantization: It's simp...  \n",
              "186  The process starts with a model that has been ...  \n",
              "187  Types of Transfer Learning include Inductive T...  \n",
              "188  Knowledge distillation involves training a sma...  \n",
              "189  Initially  a large and typically complex model...  \n",
              "\n",
              "[190 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac2e9af3-1702-48e3-ba70-5a912efd0bb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>context</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is machine learning ?</td>\n",
              "      <td>Machine learning is a branch of computer scien...</td>\n",
              "      <td>Machine learning is a branch of computer scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why do we need to know about machine learning ?</td>\n",
              "      <td>Machine Learning is an important field of data...</td>\n",
              "      <td>Machine Learning is an important field of data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is deep learning?</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How is deep learning different from machine le...</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "      <td>Unlike traditional ML  it does not require man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When do we use deep learning or machine learni...</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "      <td>Machine learning is used when the task is simp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>What are the different types of quantization</td>\n",
              "      <td>Quantization involves converting the continuou...</td>\n",
              "      <td>There is Post-Training Quantization: It's simp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>How does transfer learning work</td>\n",
              "      <td>Transfer learning involves taking a model trai...</td>\n",
              "      <td>The process starts with a model that has been ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>What are the different types of transfer learning</td>\n",
              "      <td>Types of Transfer Learning include Inductive T...</td>\n",
              "      <td>Types of Transfer Learning include Inductive T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>What is Knowledge Distillation</td>\n",
              "      <td>Knowledge distillation involves training a sma...</td>\n",
              "      <td>Knowledge distillation involves training a sma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>How does knowledge distillation work</td>\n",
              "      <td>Knowledge distillation involves training a sma...</td>\n",
              "      <td>Initially  a large and typically complex model...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>190 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac2e9af3-1702-48e3-ba70-5a912efd0bb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac2e9af3-1702-48e3-ba70-5a912efd0bb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac2e9af3-1702-48e3-ba70-5a912efd0bb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-693bb938-4716-4f6c-853f-628989f2087c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-693bb938-4716-4f6c-853f-628989f2087c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-693bb938-4716-4f6c-853f-628989f2087c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b285bb0b-4f83-4ffa-b9a4-9664ebf190ab\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b285bb0b-4f83-4ffa-b9a4-9664ebf190ab button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 190,\n  \"fields\": [\n    {\n      \"column\": \"questions\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"What are the different types of quantization \",\n          \"What is optimization in ML and deep learning \",\n          \"What is online learning ? \"\n        ],\n        \"num_unique_values\": 189,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Batch learning is the process through which the model is trained with all possible training data before deployment. Batch Learning Not capable of learning after deployment Must be retrained from scratch computationally expensiveSequence learning is a type of machine learning where the algorithm learns from sequence data The goal is to understand the structure or features of the sequence to make predictions about future elements or classify the sequence into different categories.\",\n          \"support vector machine is a supervised ML algorithm which can be used for both classification or regression It performs classification by finding the hyperplane that best divides dataset into classes Hyperplane is a generalized plane in different dimensions. When support vector machines are used for text classification problems when there is a need for margin separation for complex datasets where linear separation is not obvious the drawbacks of support vector machine includes it is inefficient on large datasets it is sensitive to noise and requires fine tuning using parameters such as the kernels support vector machines should be avoided when there is a large dataset when the dataset has a lot of noise when there is no clear margin or separation  Machine learning is used when the task is simple and structured enough and  When computational resources are minimal and  model interpretation is requiredFeature Engineering is the process of converting raw data into a numerical format that algorithms can utilize for prediction or classification Feature engineering in NLP is crucial for transforming text into  a structured machine-readable form there are many ways in which this can be achieved some of the methods include Bag of words \\u2013 countvectorizer term frequency inverse document frequency TF-IDF GloVe(Global vectors for word representation) these are the statistical ways in which texts are converted into numerical version of itself. This process can also be achieved with the help of neural networks like Word2Vec embedding SentenceBERT.   \",\n          \"In a typical recommendation system you start with a user-item matrix where rows represent users columns represent items and the values in the matrix represent user interactions with items. However this matrix is usually sparse meaning most of the values are unknown or zero as not every user interacts with every item. The goal of matrix factorization is to approximate this sparse user-item matrix by factorizing it into two lower-dimensional matrices whose product is a close approximation to the original matrix. The idea is that both items and users can be described by a small set of underlying (latent) features. Techniques like Singular Value Decomposition (SVD) Non-negative Matrix Factorization (NMF) or Alternating Least Squares (ALS) are used to decompose the original matrix into user and item matrices. By multiplying the user and item matrices you can predict the missing values in the original matrix user and item matrices These predicted values represent the user's likely preference for items they haven't interacted with. The system can then recommend items to a user based on these predicted preferences typically suggesting items with the highest predicted ratings.   TensorFlow Recommenders (TFRS) TensorFlow Recommenders is part of the TensorFlow ecosystem and is designed for building sophisticated recommendation models. It allows for a high degree of customization and is suitable for both retrieval and ranking tasksRecommendation systems are a sophisticated and integral component of modern digital environments. Their primary role is to assist users in navigating the overwhelming array of choices available in various domains such as e-commerce online streaming and social media. This personalization is not just beneficial for the user experience but also drives engagement and sales for businesses. The concept of recommendation systems is rooted in the idea of filtering large volumes of information to present the most relevant subset to the user. This aligns closely with the human desire for personalized experiences especially in an era where the amount of available information far exceeds what one can comfortably explore. \"\n        ],\n        \"num_unique_values\": 182,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Batch Learning Not capable of learning after deployment Must be retrained from scratch computationally expensiveBatch learning is the process through which the model is trained with all possible training data before deployment. \",\n          \"Gradient Boosting is a boosting algorithm that combines several weak learners into strong learners It is an ensemble method the Initialization Begins with a simple model It builds a sequence of trees where each new tree corrects the errors of its predecessors these are called residuals. Gradient boosting is used when there is an unbalanced dataset or when the model performance is the primary concern. Drawbacks of such a model is that it can overfit on noisy data requires careful tuning of parameters and Longer training time as trees are built sequentially when to avoid K nearest neighbors when time is a constraint or when the task is too simple and a simple model will suffice.          \",\n          \"MAP at k considers the order of recommendations it calculates precision at each point in the recommendation list and averages these values. For example for recommendations [01 1] the MAP at k would be 0.38 indicating the average precision across the list.\"\n        ],\n        \"num_unique_values\": 183,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset using the Dataloader and then split the dataset according to training and test set\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_sampler = RandomSampler(train_data.index)\n",
        "val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n",
        "\n",
        "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ],
      "metadata": {
        "id": "DYs6yOx0X0WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run the training loop along with model evaluation\n",
        "train_loss = 0\n",
        "val_loss = 0\n",
        "train_batch_count = 0\n",
        "val_batch_count = 0\n",
        "\n",
        "for epoch in range(2):\n",
        "    MODEL.train()\n",
        "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "        outputs = MODEL(\n",
        "                          input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels,\n",
        "                          decoder_attention_mask=decoder_attention_mask\n",
        "                        )\n",
        "\n",
        "        OPTIMIZER.zero_grad()\n",
        "        outputs.loss.backward()\n",
        "        OPTIMIZER.step()\n",
        "        train_loss += outputs.loss.item()\n",
        "        train_batch_count += 1\n",
        "\n",
        "    #Evaluation\n",
        "    MODEL.eval()\n",
        "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "        outputs = MODEL(\n",
        "                          input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels,\n",
        "                          decoder_attention_mask=decoder_attention_mask\n",
        "                        )\n",
        "\n",
        "        OPTIMIZER.zero_grad()\n",
        "        outputs.loss.backward()\n",
        "        OPTIMIZER.step()\n",
        "        val_loss += outputs.loss.item()\n",
        "        val_batch_count += 1\n",
        "\n",
        "    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
      ],
      "metadata": {
        "id": "2AynecfWX0CC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1d04b5-4973-40b2-d05f-da48fb24037f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training batches: 100%|██████████| 51/51 [17:20<00:00, 20.40s/it]\n",
            "Validation batches: 100%|██████████| 13/13 [03:51<00:00, 17.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/2 -> Train loss: 2.277066555531586\tValidation loss: 1.4941018223762512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training batches: 100%|██████████| 51/51 [16:38<00:00, 19.58s/it]\n",
            "Validation batches: 100%|██████████| 13/13 [04:14<00:00, 19.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 -> Train loss: 1.841683010333309\tValidation loss: 1.286982897669077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model weights for inference\n",
        "MODEL.save_pretrained(\"QA_model\")\n",
        "TOKENIZER.save_pretrained(\"QA_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tXTrT-TfFVi",
        "outputId": "db6dace8-f03f-49f6-ad75-c160d0473f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('QA_tokenizer/tokenizer_config.json',\n",
              " 'QA_tokenizer/special_tokens_map.json',\n",
              " 'QA_tokenizer/spiece.model',\n",
              " 'QA_tokenizer/added_tokens.json',\n",
              " 'QA_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to make model inference\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def predict_answer(context, question, ref_answer=None):\n",
        "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
        "\n",
        "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "\n",
        "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
        "\n",
        "    if ref_answer:\n",
        "        # Load the Bleu metric\n",
        "        predicted_tokens = predicted_answer.split()\n",
        "        ref_tokens = ref_answer.split()\n",
        "\n",
        "        # Compute BLEU score\n",
        "        bleu_score = sentence_bleu([ref_tokens], predicted_tokens)\n",
        "\n",
        "        print(\"Context: \\n\", context)\n",
        "        print(\"\\n\")\n",
        "        print(\"Question: \\n\", question)\n",
        "        return {\n",
        "            \"Reference Answer: \": ref_answer,\n",
        "            \"Predicted Answer: \": predicted_answer,\n",
        "            \"BLEU Score: \": bleu_score\n",
        "        }\n",
        "    else:\n",
        "        return predicted_answer"
      ],
      "metadata": {
        "id": "wG1wcNdGgQKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the inference part of the model\n",
        "import random\n",
        "n = random.randint(0,71) #asked a question at random from the dataset\n",
        "\n",
        "context = data['context'][n]\n",
        "question = data['questions'][n]\n",
        "answer = data['answer'][n]\n",
        "\n",
        "predict_answer(context, question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gew_vpjUgwkA",
        "outputId": "ddc47170-24b0-4255-97bb-df71f62fd2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: \n",
            " Instance based learning is when the model learns from only the data that is provided without any further generalization. It updates its learning with the input of new data at every instance. Machine learning algorithms are used to either make a prediction or classify a given data input. The data may or may not be labeled. The way a machine learning algorithm learns is with the help of a mathematical function which is responsible to evaluate the prediction of a model with it’s true label Models are then adjusted to reduce discrepancy between a known example and the model estimate with the help of a loss function. Model based learning models are the type of supervised models that learn underlying patterns from the data and then generalizes on new data by extending the parameters that are learned from the training data e.g. Spam filter may learn on the fly with a deep neural network – online model-based supervised learning system \n",
            "\n",
            "\n",
            "Question: \n",
            " What is instance based predictive learning ? \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Reference Answer: ': 'Instance based learning is when the model learns from only the data that is provided without any further generalization. It updates its learning with the input of new data at every instance. ',\n",
              " 'Predicted Answer: ': 'It learns from the input of new data at every instance. Instance based learning',\n",
              " 'BLEU Score: ': 0.1857843257365279}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ask a question that is not in the dataset for model inference\n",
        "def ask_RAG(query):\n",
        "  results = searcher.search(query, k=3)\n",
        "  all_data = []\n",
        "  for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    data = searcher.collection[passage_id]\n",
        "    all_data.append(data)\n",
        "    retrieved_context = ''.join(all_data)\n",
        "    prediction = predict_answer(retrieved_context, query)\n",
        "  print(f\"retrieved information: {retrieved_context} \\ngenerated response: {prediction}\")\n"
      ],
      "metadata": {
        "id": "dX2V7oDleiJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try out the model"
      ],
      "metadata": {
        "id": "sASlAksR1SOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the type of sample questions we can ask the model\n",
        "\n",
        "import random\n",
        "for i in range(10):\n",
        "  n = random.randint(0,len(questions)-1)\n",
        "  print(questions[n])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcbynvG-DrJv",
        "outputId": "795743a9-e157-45d6-f59b-fc9e96f20403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is collaborative filtering ?\n",
            "What are the computational challenges faced by a CNN model \n",
            "What is dropout method in model optimization \n",
            "What is Mean squared bias in regression analysis?\n",
            "What is forward pass in the training process of a convolutional network \n",
            "What are the different methods of hyperparameter tuning ?\n",
            "What is context aware systems ?\n",
            "What is byte pair encoding \n",
            "What are the different types of recommendation systems \n",
            "How can the computational costs be reduced for a CNN model ? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training question\n",
        "query = \"What is byte pair encoding ?\"\n",
        "ask_RAG(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8nRezExLqMa",
        "outputId": "8a60e059-e33c-4f5a-c91c-60a47ea166a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieved information: Byte Pair Encoding (BPE) is a middle ground between word-level and character-level tokenization. It starts with a base vocabulary of individual characters and iteratively merges the most frequent pair of tokens Efficient in representing common subword units significantly reduces the vocabulary size and handles out-of-vocabulary words well. Used in models like GPT-2 GPT-3The Seq2seq model uses encode and decoder architecture. Where the encoder Processes the input sequence and compresses the information into a context vector. The decoder then takes the context vector and generates the output sequence. Seq2seq models are typically trained end-to-end on paired\n",
            "sequences (e.g.  an English sentence and its French translation) The training objective is to maximize the likelihood of the correct output sequence given the input sequence. Seq2seq models are good at translating text from one language to another  converting spoken language into text  text summarization  and in chatbots. There are different ways to implement positional encoding but one common approach involves using sine and cosine functions of different frequencies. For each position i and dimension d in the embedding space a positional encoding vector P is defined. The reason why a sinusoidal function is used is because of Predictability(It allows the model to easily learn to attend by relative positions) Scalability (The sinusoidal pattern is not limited to a fixed length of the sequence)  Efficiency: (Using sinusoids allows for more efficient learning and easier extrapolation to longer sequence lengths) \n",
            "generated response: Byte Pair Encoding (BPE) is a. Byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Within syllabus question but paraphrased\n",
        "query = \"Can you tell me something about collaborative filtering ?\"\n",
        "ask_RAG(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYqsfQCoIhs-",
        "outputId": "c6caf659-0b11-4143-aa37-45a9c6a45daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieved information: There are two types of collaborative filtering. User-Based Collaborative Filtering: This method finds users who have similar preferences or behavior patterns to the target user and recommends items that these similar users have liked or interacted with and Item-Based Collaborative Filtering: Instead of finding similar users this approach identifies items that are similar to those the user has already liked or interacted with based on other users' interactions with these items.    Hybrid recommendation systems combine collaborative and content-based filtering methods. The goal is to improve recommendation quality and overcome the limitations inherent in any single approach. For example a hybrid system might use collaborative filtering to identify a set of users with similar tastes and then use content-based filtering to find items that those users liked and that match the target user's content preferences.The different type of recommendation system includes Collaborative filtering. Content based filtering demographic based filtering context aware systems hybrid systems and many more.  \n",
            "generated response: There are two types of collaborative filtering: User-Based Collaborative Filtering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Out of syllabus question but within the domain\n",
        "query = \"How can Machine Learning help us in life?\"\n",
        "ask_RAG(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Xxg-Z7IuKQ",
        "outputId": "e3889bda-80d5-4316-ab12-31b8a804272b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieved information: Machine learning is a branch of computer science which focuses on the use of data and algorithms to imitate the way humans learn Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  Machine learning algorithms are used to either make a prediction or classify a given data input. The data may or may not be labeled. The way a machine learning algorithm learns is with the help of a mathematical function which is responsible to evaluate the prediction of a model with it’s true label Models are then adjusted to reduce discrepancy between a known example and the model estimate with the help of a loss function.  \n",
            "generated response: is a branch of computer science which focuses on the use of data and algorithms to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Out of syllabus question and out of domain\n",
        "query = \"How to be happy in life?\"\n",
        "ask_RAG(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0r289-BJMfY",
        "outputId": "3335f06b-1325-4188-d40b-aaf4fc18b2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieved information: Precision counts the true positives out of all the items predicted to be positive Ideal Usage When the cost of false positive is too high Example: Email spam detectionA perfect model fit can be achieved By first constructing good evaluation metrics to give us feedback on model performance Then tuning hyper-parameters till the performance improves across the board Recall counts how many of the true positive items were correctly classified Ideal Usage When the missing a positive is too costly  \n",
            "generated response: :         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with Langchain GPT-3.5\n",
        "\n",
        "NOTE: This requires an API key which will be available for 3 months starting from 14th Feb 2024"
      ],
      "metadata": {
        "id": "VvFbYvngdJ5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "O4J1aL_Bc28u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "4DiIQlEGfMmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "xAG2nc5yerR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1p_dYRgzwZ",
        "outputId": "45989861-5f50-41f9-b496-1ebc9cbf1ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=\"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\")"
      ],
      "metadata": {
        "id": "YrY7juwcg_Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "yzHqGn8uh8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try out the Langchain framework with GPT 3.5  \n",
        "Please note that this will work only until the OpenAi API is free."
      ],
      "metadata": {
        "id": "wKaPsY22MpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you tell me something about collaborative filtering ?\""
      ],
      "metadata": {
        "id": "AbFvKujHMMdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "results = searcher.search(query, k=3)\n",
        "all_data = []\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "  data = searcher.collection[passage_id]\n",
        "  all_data.append(data)\n",
        "  retrieved_context = ''.join(all_data)\n",
        "\n",
        "chain.invoke({\"context\":retrieved_context,\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exii7ZbCiCqD",
        "outputId": "98401f9f-40a3-4d2e-89a0-14ce2cc3c174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid auth\"}\\')')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Collaborative filtering is a recommendation system that identifies users with similar preferences or behavior patterns and recommends items based on the interactions of these similar users. It can be user-based, where similar users are found, or item-based, where similar items are identified. Hybrid recommendation systems combine collaborative and content-based filtering methods to improve recommendation quality.')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid auth\"}\\')')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the secret to a successful life?\"\n",
        "# Run\n",
        "results = searcher.search(query, k=3)\n",
        "all_data = []\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "  data = searcher.collection[passage_id]\n",
        "  all_data.append(data)\n",
        "  retrieved_context = ''.join(all_data)\n",
        "\n",
        "chain.invoke({\"context\":retrieved_context,\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-YZzRGDNUCM",
        "outputId": "c43c942e-3e12-4088-a8d6-19bf8a32c0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid auth\"}\\')')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The context provided does not address the question of what the secret to a successful life is.')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid auth\"}\\')')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8v1J2I2uap5o",
        "VvFbYvngdJ5i"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97c8e1448c6541c69e391efdcfe29ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05014f3cac2744dfac20f383a5e26054",
              "IPY_MODEL_15b973809e424ae097506562ecc8295f",
              "IPY_MODEL_7f014d52e21c44d282d5807e23cc4f85"
            ],
            "layout": "IPY_MODEL_4ee5b8255da34544aba12175b33e85da"
          }
        },
        "05014f3cac2744dfac20f383a5e26054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c08cb8731234e5c8d04913a0e763e11",
            "placeholder": "​",
            "style": "IPY_MODEL_51adfa501baa4056948e3f539622dc64",
            "value": "artifact.metadata: 100%"
          }
        },
        "15b973809e424ae097506562ecc8295f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473fc769367b4093808c116125b0552b",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ece8c5dccbe94537a68a6d0f9fbeecb7",
            "value": 1633
          }
        },
        "7f014d52e21c44d282d5807e23cc4f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bbd2385204f4ca0b967b53b6acb2183",
            "placeholder": "​",
            "style": "IPY_MODEL_567a627e6d3c485f924a114e4033c15d",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 28.0kB/s]"
          }
        },
        "4ee5b8255da34544aba12175b33e85da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c08cb8731234e5c8d04913a0e763e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51adfa501baa4056948e3f539622dc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "473fc769367b4093808c116125b0552b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece8c5dccbe94537a68a6d0f9fbeecb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bbd2385204f4ca0b967b53b6acb2183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567a627e6d3c485f924a114e4033c15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}